{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HiFi-GAN Vocoder Demo\n",
        "\n",
        "This notebook demonstrates the HiFi-GAN vocoder trained on RUSLAN dataset for Russian TTS.\n",
        "\n",
        "**Paper:** [HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis](https://arxiv.org/abs/2010.05646)\n",
        "\n",
        "**Training Logs:** [CometML](https://www.comet.com/krugd/hifigan-russian-tts/444cc4e3c69c439cbd2f2186e108e03c)\n",
        "\n",
        "## Contents\n",
        "1. Setup and Installation\n",
        "2. Download Pre-trained Weights\n",
        "3. Download Test Audio\n",
        "4. Load Model and Synthesize\n",
        "5. Resynthesize Mode Demo\n",
        "6. Batch Synthesis with synthesize.py\n",
        "7. Full TTS Pipeline (Optional)\n",
        "8. Performance Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check environment\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "print(f\"Running in Google Colab: {IN_COLAB}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clone repository (Colab only)\n",
        "if IN_COLAB:\n",
        "    !git clone https://github.com/KrugD/HW3_TTS.git\n",
        "    %cd HW3_TTS\n",
        "else:\n",
        "    print(\"Running locally - skipping clone\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies from requirements.txt\n",
        "!pip install -q -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup paths\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "if IN_COLAB:\n",
        "    PROJECT_ROOT = Path('/content/HW3_TTS')\n",
        "else:\n",
        "    PROJECT_ROOT = Path('.').absolute()\n",
        "\n",
        "sys.path.insert(0, str(PROJECT_ROOT))\n",
        "os.chdir(PROJECT_ROOT)\n",
        "\n",
        "print(f\"Project root: {PROJECT_ROOT}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "import gdown\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchaudio\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "# Import project modules (requires PROJECT_ROOT in sys.path)\n",
        "from src.model import Generator\n",
        "from src.transforms import MelSpectrogram, MelSpectrogramConfig\n",
        "\n",
        "print(\"All imports loaded successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Download Pre-trained Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directories\n",
        "WEIGHTS_DIR = PROJECT_ROOT / 'models_weights'\n",
        "TEST_DIR = PROJECT_ROOT / 'data' / 'test' / 'audio'\n",
        "OUTPUT_DIR = PROJECT_ROOT / 'output' / 'synthesized'\n",
        "\n",
        "WEIGHTS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "TEST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download weights from Google Drive\n",
        "WEIGHTS_PATH = WEIGHTS_DIR / 'generator_best.pt'\n",
        "WEIGHTS_FILE_ID = \"1LTl3m5ZScfefGvQUXTBAYfBeoSKhDZR2\"\n",
        "\n",
        "if not WEIGHTS_PATH.exists():\n",
        "    print(\"Downloading HiFi-GAN generator weights...\")\n",
        "    gdown.download(\n",
        "        f\"https://drive.google.com/uc?id={WEIGHTS_FILE_ID}\",\n",
        "        str(WEIGHTS_PATH),\n",
        "        quiet=False\n",
        "    )\n",
        "    print(f\"Downloaded to: {WEIGHTS_PATH}\")\n",
        "else:\n",
        "    print(f\"Weights already exist: {WEIGHTS_PATH}\")\n",
        "\n",
        "# Verify file size\n",
        "if WEIGHTS_PATH.exists():\n",
        "    size_mb = WEIGHTS_PATH.stat().st_size / (1024 * 1024)\n",
        "    print(f\"File size: {size_mb:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Download Test Audio from RUSLAN Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download a few samples from RUSLAN for testing\n",
        "# RUSLAN dataset: https://ruslan-corpus.github.io/\n",
        "\n",
        "# We'll download a small subset for demo purposes\n",
        "# You can also provide your own .wav files in data/test/audio/\n",
        "\n",
        "RUSLAN_SAMPLE_URL = \"https://ruslan-corpus.github.io/data/ruslan_sample.zip\"\n",
        "\n",
        "# Check if we already have test files\n",
        "existing_files = list(TEST_DIR.glob('*.wav'))\n",
        "print(f\"Found {len(existing_files)} existing test audio files\")\n",
        "\n",
        "if len(existing_files) == 0:\n",
        "    print(\"\\nNo test audio found. You can:\")\n",
        "    print(\"1. Upload your own .wav files to data/test/audio/\")\n",
        "    print(\"2. Download RUSLAN dataset with: python download_ruslan.py\")\n",
        "    print(\"\\nFor this demo, we'll create a simple test audio.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If no test files, allow upload or use synthetic audio for structure demo\n",
        "if IN_COLAB and len(list(TEST_DIR.glob('*.wav'))) == 0:\n",
        "    from google.colab import files\n",
        "    print(\"Upload .wav files for testing (or skip to use synthetic audio):\")\n",
        "    try:\n",
        "        uploaded = files.upload()\n",
        "        for filename, content in uploaded.items():\n",
        "            with open(TEST_DIR / filename, 'wb') as f:\n",
        "                f.write(content)\n",
        "            print(f\"Saved: {filename}\")\n",
        "    except:\n",
        "        print(\"Upload skipped or failed\")\n",
        "\n",
        "# Fallback: create synthetic test audio if needed\n",
        "if len(list(TEST_DIR.glob('*.wav'))) == 0:\n",
        "    print(\"\\nCreating synthetic test audio for demo structure...\")\n",
        "    print(\"Note: For real quality testing, use actual speech recordings!\")\n",
        "    \n",
        "    # Create a simple tone (just to show the pipeline works)\n",
        "    sr = 22050\n",
        "    duration = 2.0\n",
        "    t = np.linspace(0, duration, int(sr * duration), dtype=np.float32)\n",
        "    # Simple harmonic tone\n",
        "    audio = 0.5 * np.sin(2 * np.pi * 220 * t) + 0.3 * np.sin(2 * np.pi * 440 * t)\n",
        "    audio = audio * np.exp(-t * 0.5)  # Decay envelope\n",
        "    audio = torch.from_numpy(audio).unsqueeze(0)\n",
        "    torchaudio.save(str(TEST_DIR / 'test_tone.wav'), audio, sr)\n",
        "    print(f\"Created: {TEST_DIR / 'test_tone.wav'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model and Synthesize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set device\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create mel-spectrogram transform (same config as training)\n",
        "mel_config = MelSpectrogramConfig(\n",
        "    sr=22050,\n",
        "    n_fft=1024,\n",
        "    win_length=1024,\n",
        "    hop_length=256,\n",
        "    n_mels=80,\n",
        "    f_min=0.0,\n",
        "    f_max=8000.0,\n",
        ")\n",
        "mel_transform = MelSpectrogram(mel_config).to(device)\n",
        "print(\"Mel-spectrogram transform created\")\n",
        "print(f\"  Sample rate: {mel_config.sr} Hz\")\n",
        "print(f\"  Mel bands: {mel_config.n_mels}\")\n",
        "print(f\"  Hop length: {mel_config.hop_length}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load HiFi-GAN Generator V1\n",
        "generator = Generator(\n",
        "    in_channels=80,\n",
        "    hidden_channels=512,\n",
        "    upsample_rates=(8, 8, 2, 2),\n",
        "    upsample_kernel_sizes=(16, 16, 4, 4),\n",
        "    resblock_kernel_sizes=(3, 7, 11),\n",
        "    resblock_dilations=((1, 3, 5), (1, 3, 5), (1, 3, 5)),\n",
        ")\n",
        "\n",
        "# Load weights\n",
        "checkpoint = torch.load(WEIGHTS_PATH, map_location=device)\n",
        "if isinstance(checkpoint, dict) and 'generator' in checkpoint:\n",
        "    generator.load_state_dict(checkpoint['generator'])\n",
        "else:\n",
        "    generator.load_state_dict(checkpoint)\n",
        "\n",
        "generator = generator.to(device)\n",
        "generator.eval()\n",
        "generator.remove_weight_norm()  # Faster inference\n",
        "\n",
        "# Count parameters\n",
        "n_params = sum(p.numel() for p in generator.parameters())\n",
        "print(f\"\\nGenerator loaded successfully!\")\n",
        "print(f\"Parameters: {n_params:,} (~{n_params/1e6:.1f}M)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Resynthesize Mode Demo\n",
        "\n",
        "In this mode, we:\n",
        "1. Load ground-truth audio\n",
        "2. Extract mel-spectrogram\n",
        "3. Generate audio from mel-spectrogram\n",
        "4. Compare original vs synthesized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def resynthesize(audio_path, generator, mel_transform, device):\n",
        "    \"\"\"\n",
        "    Resynthesize audio: load -> mel -> vocoder -> audio\n",
        "    \n",
        "    Returns:\n",
        "        original audio, generated audio, mel spectrogram, synthesis time\n",
        "    \"\"\"\n",
        "    # Load and preprocess audio\n",
        "    audio, sr = torchaudio.load(audio_path)\n",
        "    if audio.shape[0] > 1:\n",
        "        audio = audio.mean(dim=0, keepdim=True)\n",
        "    audio = audio.squeeze(0)\n",
        "    \n",
        "    # Resample if needed\n",
        "    if sr != mel_config.sr:\n",
        "        resampler = torchaudio.transforms.Resample(sr, mel_config.sr)\n",
        "        audio = resampler(audio)\n",
        "    \n",
        "    # Normalize\n",
        "    if audio.abs().max() > 0:\n",
        "        audio = audio / audio.abs().max()\n",
        "    \n",
        "    # Extract mel-spectrogram\n",
        "    mel = mel_transform(audio.to(device))\n",
        "    \n",
        "    # Synthesize\n",
        "    start_time = time.time()\n",
        "    audio_gen = generator(mel.unsqueeze(0))\n",
        "    synthesis_time = time.time() - start_time\n",
        "    \n",
        "    audio_gen = audio_gen.squeeze().cpu()\n",
        "    \n",
        "    # Normalize output\n",
        "    if audio_gen.abs().max() > 0:\n",
        "        audio_gen = audio_gen / audio_gen.abs().max() * 0.95\n",
        "    \n",
        "    return audio.cpu(), audio_gen, mel.cpu(), synthesis_time\n",
        "\n",
        "\n",
        "def plot_comparison(audio_orig, audio_gen, mel, sr=22050):\n",
        "    \"\"\"Plot waveforms and spectrogram comparison.\"\"\"\n",
        "    fig, axes = plt.subplots(3, 1, figsize=(14, 8))\n",
        "    \n",
        "    # Time axis\n",
        "    t_orig = torch.arange(len(audio_orig)) / sr\n",
        "    t_gen = torch.arange(len(audio_gen)) / sr\n",
        "    \n",
        "    # Original waveform\n",
        "    axes[0].plot(t_orig.numpy(), audio_orig.numpy(), linewidth=0.5)\n",
        "    axes[0].set_title('Original Audio Waveform')\n",
        "    axes[0].set_xlabel('Time (s)')\n",
        "    axes[0].set_ylabel('Amplitude')\n",
        "    axes[0].set_xlim([0, t_orig[-1]])\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Generated waveform\n",
        "    axes[1].plot(t_gen.numpy(), audio_gen.numpy(), linewidth=0.5, color='orange')\n",
        "    axes[1].set_title('Generated Audio Waveform (HiFi-GAN)')\n",
        "    axes[1].set_xlabel('Time (s)')\n",
        "    axes[1].set_ylabel('Amplitude')\n",
        "    axes[1].set_xlim([0, t_gen[-1]])\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    \n",
        "    # Mel-spectrogram\n",
        "    im = axes[2].imshow(mel.numpy(), aspect='auto', origin='lower', cmap='viridis')\n",
        "    axes[2].set_title('Mel-Spectrogram (Input to Vocoder)')\n",
        "    axes[2].set_xlabel('Time Frames')\n",
        "    axes[2].set_ylabel('Mel Frequency Bin')\n",
        "    plt.colorbar(im, ax=axes[2], label='Log Magnitude')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Process all test audio files\n",
        "audio_files = sorted(TEST_DIR.glob('*.wav'))\n",
        "print(f\"Found {len(audio_files)} test audio files\\n\")\n",
        "\n",
        "results = []\n",
        "\n",
        "for audio_path in audio_files:\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"File: {audio_path.name}\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Resynthesize\n",
        "    audio_orig, audio_gen, mel, synth_time = resynthesize(\n",
        "        audio_path, generator, mel_transform, device\n",
        "    )\n",
        "    \n",
        "    # Calculate metrics\n",
        "    audio_duration = len(audio_gen) / mel_config.sr\n",
        "    rtf = synth_time / audio_duration\n",
        "    speed = 1 / rtf\n",
        "    \n",
        "    print(f\"Audio duration: {audio_duration:.2f} s\")\n",
        "    print(f\"Synthesis time: {synth_time*1000:.1f} ms\")\n",
        "    print(f\"Real-time factor: {rtf:.4f}\")\n",
        "    print(f\"Speed: {speed:.1f}x faster than real-time\")\n",
        "    \n",
        "    results.append({\n",
        "        'file': audio_path.name,\n",
        "        'duration': audio_duration,\n",
        "        'synth_time': synth_time,\n",
        "        'rtf': rtf,\n",
        "        'speed': speed\n",
        "    })\n",
        "    \n",
        "    # Plot comparison\n",
        "    plot_comparison(audio_orig, audio_gen, mel)\n",
        "    \n",
        "    # Play audio\n",
        "    print(\"\\nOriginal Audio:\")\n",
        "    display(Audio(audio_orig.numpy(), rate=mel_config.sr))\n",
        "    \n",
        "    print(\"\\nGenerated Audio (HiFi-GAN):\")\n",
        "    display(Audio(audio_gen.numpy(), rate=mel_config.sr))\n",
        "    \n",
        "    # Save generated audio\n",
        "    output_path = OUTPUT_DIR / f\"{audio_path.stem}_generated.wav\"\n",
        "    torchaudio.save(str(output_path), audio_gen.unsqueeze(0), mel_config.sr)\n",
        "    print(f\"\\nSaved to: {output_path}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Batch Synthesis with synthesize.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate synthesize.py script usage\n",
        "print(\"Running synthesize.py for batch processing...\")\n",
        "print()\n",
        "\n",
        "!python synthesize.py \\\n",
        "    input_dir=data/test/audio \\\n",
        "    output_dir=output/batch_synthesized \\\n",
        "    checkpoint=models_weights/generator_best.pt \\\n",
        "    resynthesize=true \\\n",
        "    device={device}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List generated files\n",
        "batch_output = PROJECT_ROOT / 'output' / 'batch_synthesized'\n",
        "if batch_output.exists():\n",
        "    print(\"Generated files:\")\n",
        "    for f in sorted(batch_output.glob('*.wav')):\n",
        "        size = f.stat().st_size / 1024\n",
        "        print(f\"  {f.name} ({size:.1f} KB)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Performance Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "if results:\n",
        "    df = pd.DataFrame(results)\n",
        "    print(\"Performance Summary\")\n",
        "    print(\"=\" * 50)\n",
        "    print(df.to_string(index=False))\n",
        "    print()\n",
        "    print(f\"Average speed: {df['speed'].mean():.1f}x real-time\")\n",
        "    print(f\"Average RTF: {df['rtf'].mean():.4f}\")\n",
        "else:\n",
        "    print(\"No results to summarize\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model info summary\n",
        "print(\"\\nModel Information\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Model: HiFi-GAN V1\")\n",
        "print(f\"Parameters: {n_params:,}\")\n",
        "print(f\"Device: {device}\")\n",
        "print(f\"Sample Rate: {mel_config.sr} Hz\")\n",
        "print(f\"Mel Bands: {mel_config.n_mels}\")\n",
        "print(f\"Hop Length: {mel_config.hop_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusion\n",
        "\n",
        "This demo showed:\n",
        "\n",
        "1. **Setup**: Cloning repo and installing dependencies\n",
        "2. **Model Loading**: Loading pre-trained HiFi-GAN V1 from Google Drive\n",
        "3. **Resynthesize Mode**: Ground-truth audio -> mel -> vocoder -> audio\n",
        "4. **Batch Processing**: Using `synthesize.py` script\n",
        "5. **Full TTS**: Integration with acoustic models (conceptual)\n",
        "6. **Performance**: Real-time factor and speed metrics\n",
        "\n",
        "For more details, see:\n",
        "- [README.md](https://github.com/KrugD/HW3_TTS?tab=readme-ov-file)\n",
        "- [HiFi-GAN Paper](https://arxiv.org/abs/2010.05646)\n",
        "- [CometML Training Logs](https://www.comet.com/krugd/hifigan-russian-tts)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
